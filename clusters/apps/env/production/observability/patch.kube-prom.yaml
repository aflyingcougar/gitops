---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus
spec:
  values:
    alertmanager:
      config:
        receivers:
          - name: "null"
          - name: "discord"
            slack_configs:
              - channel: "#prometheus"
                icon_url: https://avatars3.githubusercontent.com/u/3380462
                username: "Prometheus - Home"
                send_resolved: true
                title: |-
                  [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }} {{ else if ne .CommonAnnotations.message ""}}{{ .CommonAnnotations.message }} {{ else if ne .CommonAnnotations.description ""}}{{ .CommonAnnotations.description }} {{ else }}{{ .CommonLabels.alertname }}{{ end }}
                text: >-
                  {{ range .Alerts -}}
                    *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
                  {{ if ne .Annotations.summary ""}}*Summary:* {{ .Annotations.summary }} {{ else if ne .Annotations.message ""}}*Message:* {{ .Annotations.message }} {{ else if ne .Annotations.description ""}}*Description:* {{ .Annotations.description }}{{ end }}
                  *Details:*
                    {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                    {{ end }}
                  {{ end }}
        global:
          resolve_timeout: 5m
          slack_api_url: ${SLACK_WEBHOOK_URL}
        route:
          group_by: ["alertname", "job"]
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 6h
          receiver: "discord"
          routes:
            - receiver: "null"
              matchers:
                - alertname =~ "InfoInhibitor|Watchdog"
            - receiver: "discord"
              match_re:
                # severity: critical|warning
                severity: critical
              continue: true
        templates:
          - '/etc/alertmanager/config/*.tmpl'
      ingress:
        enabled: true
        ingressClassName: traefik-internal
        annotations:
          hajimari.io/enable: "true"
          hajimari.io/icon: "bell-alert-outline"
          hajimari.io/appName: "Alertmanager"
          hajimari.io/group: "Observability"
        labels: {}
        hosts:
          - "alerts.${SECRET_DOMAIN}"
        paths:
          - /
        pathType: ImplementationSpecific
        tls:
          - secretName: crj-wildcard-certificate
            hosts:
              - "alerts.${SECRET_DOMAIN}"
      alertmanagerSpec:
        podMetadata:
          annotations:
            linkerd.io/inject: enabled
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-block
              resources:
                requests:
                  storage: 1Gi
        resources:
          requests:
            cpu: 11m
            memory: 50M
          limits:
            memory: 99M
    ##
    ## Grafana
    ##
    grafana:
      enabled: false
    ##
    ## Component Configs
    ##
    kubeApiServer:
      enabled: true

    kubelet:
      enabled: true
      serviceMonitor:
        metricRelabelings:
          - action: replace
            sourceLabels:
              - node
            targetLabel: instance

    kubeControllerManager:
      enabled: true
      endpoints:
        - 192.168.130.31
        - 192.168.130.32
        - 192.168.130.33
      service:
        enabled: true
        port: 10257
        targetPort: 10257
      serviceMonitor:
        enabled: true
        https: true
        insecureSkipVerify: true

    coreDns:
      enabled: true
      service:
        port: 9153
        targetPort: 9153

    kubeEtcd:
      enabled: true
      endpoints:
        - 192.168.130.31
        - 192.168.130.32
        - 192.168.130.33
      service:
        enabled: true
        port: 2381
        targetPort: 2381

    kubeScheduler:
      enabled: true
      endpoints:
        - 192.168.130.31
        - 192.168.130.32
        - 192.168.130.33
      service:
        enabled: true
        port: 10249
        targetPort: 10249

    kubeProxy:
      enabled: false
      endpoints:
        - 192.168.130.31
        - 192.168.130.32
        - 192.168.130.33

    kubeStateMetrics:
      enabled: true

    kube-state-metrics:
      metricLabelsAllowlist:
        - "persistentvolumeclaims=[*]"
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: kubernetes_node

    prometheus-node-exporter:
      podAnnotations:
        linkerd.io/inject: enabled
    ## Prometheus Operator
    #
    prometheusOperator:
      admissionWebhooks:
        patch:
          resources:
            limits:
              cpu: 100m
              memory: 150Mi
            requests:
              cpu: 50m
              memory: 50Mi
      resources:
        limits:
          cpu: 200m
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 100Mi
      prometheusConfigReloader:
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
          limits:
            cpu: 100m
            memory: 50Mi
    ## Prometheus instance
    #
    prometheus:
      ingress:
        enabled: true
        ingressClassName: traefik-internal
        annotations:
          hajimari.io/enable: "true"
          hajimari.io/icon: "fire"
          hajimari.io/appName: "Prometheus"
          hajimari.io/group: "Observability"
        labels: {}
        hosts:
          - prometheus.${SECRET_DOMAIN}
        paths:
          - /
        pathType: ImplementationSpecific
        tls:
          - secretName: crj-wildcard-certificate
            hosts:
              - prometheus.${SECRET_DOMAIN}
      prometheusSpec:
        replicas: 1
        replicaExternalLabelName: "replica"
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        retentionSize: "6GB"
        retention: 2d
        enableAdminAPI: true
        walCompression: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: ceph-block
              resources:
                requests:
                  storage: 10Gi
        podMetadata:
          annotations:
            linkerd.io/inject: enabled
        resources:
          requests:
            cpu: 587m
            memory: 4993M
          limits:
            memory: 5977M
        additionalScrapeConfigs:
          - job_name: 'prometheus'
            static_configs:
              - targets: ['localhost:9090']
          - job_name: 'linkerd-controller'
            kubernetes_sd_configs:
              - role: pod
                namespaces:
                  names:
                    - 'linkerd'
                    - 'linkerd-viz'
            relabel_configs:
              - source_labels:
                  - __meta_kubernetes_pod_container_port_name
                action: keep
                regex: admin-http
              - source_labels: [__meta_kubernetes_pod_container_name]
                action: replace
                target_label: component
          - job_name: 'linkerd-service-mirror'
            kubernetes_sd_configs:
              - role: pod
            relabel_configs:
              - source_labels:
                  - __meta_kubernetes_pod_label_component
                  - __meta_kubernetes_pod_container_port_name
                action: keep
                regex: linkerd-service-mirror;admin-http$
              - source_labels: [__meta_kubernetes_pod_container_name]
                action: replace
                target_label: component
          - job_name: 'linkerd-proxy'
            kubernetes_sd_configs:
              - role: pod
            relabel_configs:
              - source_labels:
                  - __meta_kubernetes_pod_container_name
                  - __meta_kubernetes_pod_container_port_name
                  - __meta_kubernetes_pod_label_linkerd_io_control_plane_ns
                action: keep
                regex: ^linkerd-proxy;linkerd-admin;linkerd$
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: namespace
              - source_labels: [__meta_kubernetes_pod_name]
                action: replace
                target_label: pod
              # special case k8s' "job" label, to not interfere with prometheus' "job"
              # label
              # __meta_kubernetes_pod_label_linkerd_io_proxy_job=foo =>
              # k8s_job=foo
              - source_labels: [__meta_kubernetes_pod_label_linkerd_io_proxy_job]
                action: replace
                target_label: k8s_job
              # drop __meta_kubernetes_pod_label_linkerd_io_proxy_job
              - action: labeldrop
                regex: __meta_kubernetes_pod_label_linkerd_io_proxy_job
              # __meta_kubernetes_pod_label_linkerd_io_proxy_deployment=foo =>
              # deployment=foo
              - action: labelmap
                regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
              # drop all labels that we just made copies of in the previous labelmap
              - action: labeldrop
                regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
              # __meta_kubernetes_pod_label_linkerd_io_foo=bar =>
              # foo=bar
              - action: labelmap
                regex: __meta_kubernetes_pod_label_linkerd_io_(.+)
              # Copy all pod labels to tmp labels
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
                replacement: __tmp_pod_label_$1
              # Take `linkerd_io_` prefixed labels and copy them without the prefix
              - action: labelmap
                regex: __tmp_pod_label_linkerd_io_(.+)
                replacement: __tmp_pod_label_$1
              # Drop the `linkerd_io_` originals
              - action: labeldrop
                regex: __tmp_pod_label_linkerd_io_(.+)
              # Copy tmp labels into real labels
              - action: labelmap
                regex: __tmp_pod_label_(.+)
