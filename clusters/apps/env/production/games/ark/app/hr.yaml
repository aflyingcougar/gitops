---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: ark
spec:
  interval: 5m
  chart:
    spec:
      chart: ark-cluster
      version: 0.2.1
      sourceRef:
        kind: HelmRepository
        name: ark-server-charts
        namespace: flux-system
  values:
    nameOverride: ""
    fullnameOverride: ""
    commonLabels: {}
    commonAnnotations:
      io.cilium/lb-ipam-ips: "${CLUSTER_LB_ARK_SERVER}"
      io.cilium/lb-ipam-sharing-key: ark-cluster
    nodeSelector:
      ${FAMILY_DOMAIN}/role: gameserver
    affinity: {}
    tolerations: []
    securityContext:
      runAsUser: 1000
      fsGroup: 1000
    podLabels: {}
    podAnnotations:
      reloader.stakater.com/auto: "true"
    podSecurityContext: {}
      # capabilities:
      #   drop:
      #     - ALL
      #   add:
      #     - CHOWN
      #     - NET_BIND_SERVICE
      # readOnlyRootFilesystem: true
      # runAsNonRoot: false
    topologySpreadConstraints: {}

    image:
      registry: docker.io
      repository: drpsychick/arkserver
      tag: lunar
      pullSecrets: []
      pullPolicy: Always

    updateStrategy:
      type: Recreate

    restartPolicy: Always

    # Time for the server to shutdown gracefully
    terminationGracePeriodSeconds: 60

    # Defaults to 1, we set this to 0 so servers don't start automatically after deploy
    #
    replicaCount: 1

    # Attach pod directly to host network
    # Implies: `service.enabled: false`
    # Mutually exclusive: `hostPort`
    hostNetwork: false

    # Attach specific ports to host node.
    # Implies: `service.enabled: false`
    # Mutually exclusive: `hostNetwork`
    hostPort: false

    # set to "ClusterFirstWithHostNet" when hostNetwork=true
    #
    # dnsPolicy: ClusterFirst

    # Cluster name
    clusterName: fleetwood

    # Mods available in the cluster and enabled by default on all servers.
    # Mods are updated with the game and can be overwritten per server.
    mods: []
    #  - "731604991"
    #  - "889745138"
    #  - "893904615"
    #  - "1404697612"
    #  - "621154190"
    #  - "564895376"
    #  - "931434275"

    # Set RCON password for the whole cluster
    rcon:
      password: ${ARK_SERVER_RCON_PASSWORD}

    # Global extraEnvVars for all servers
    extraEnvVars:
      - name: am_arkwarnminutes
        value: "15"

    # Global custom game settings can be overwritten per server
    #customConfigMap:
    # GameIni: ark-server-global-gameini
    # GameUserSettingsIni: ark-server-global-user-settings-ini
    # EngineIni: ark-server-global-engine-ini
    # crontab: ""

    # Servers in the ARK cluster
    servers:
      # one entry for each server in the cluster
      theisland:
        # updateOnStart should be enabled only on the first server
        updateOnStart: true
        sessionName: "The Island"
        message: "Welcome to The Island"
        # map: TheIsland, Ragnarok, CrystalIsles, Aberration_P, ScorchedEarth_P, Extinction, ...
        map: TheIsland
        password: "${ARK_SERVER_PASSWORD}"
        maxPlayers: 10
        # xpMultiplier is added to default GameUserSettings.ini
        # if you use `customConfigMap.GameUserSettingsIni` make sure to include it there
      #  xpMultiplier: 6
        extraEnvVars:
          - name: am_arkwarnminutes
            value: "15"
        # ports must be the same on external and internal
        ## we don't need a service abtraction as every pod is a single server with dedicated ports
        # a service with nodePort would be "right" configuration, but for latency reasons I'd skip it.
        # with service: public:30200 -> [nodeport:30200] -> service:$gameudp -> pod:30200
        # hostnetwork (no service!): public:30200 -> node=pod:30200
        # pod hostPort (through service): public:30200 -> node=pod:30200
        # difference hostPort hostNetwork : Port only exposes a single port
        ports:
          queryudp: 27010
          gameudp: 7770
          rcon: 32330
        # override mods for a single server
        # mods: []
        rcon:
          password: "${ARK_SERVER_RCON_PASSWORD}"
        resources:
          requests:
            cpu: 2
            memory: 8Gi
          limits:
            memory: 16Gi
        # customConfigMap:
        #   GameIni: |
        #     # Extinction Game.ini
        #   GameUserSettingsIni: |
        #     # Extinction GameUserSettings.ini
        #     [ServerSettings]
        #     XPMultiplier=6
        #   EngineIni: |
        #     # Extinction Engine.ini

    # Containers' resource requests and limits
    # ref: http://kubernetes.io/docs/user-guide/compute-resources/
    #
    resources:
      requests:
        cpu: 2
        memory: 8Gi
      limits:
        memory: 16Gi

    service:
      enabled: true
      # IPAM is handled by the global annotations
      type: LoadBalancer

    persistence:
      enabled: true
      existingVolumes: true
      # volume is shared verbatim across all deployments
      game:
        existingClaim: ark-game
        mountPath: /arkserver
      # volume is shared verbatim across all deployments
      cluster:
        mountPath: /arkserver/ShooterGame/Saved/clusters
        existingClaim: ark-cluster
      # existingClaim name is appended with server name
      # ark-save will become ark-save-theisland or ark-save-extinction
      # these volumes will have to be created manually every time a new server is spun up
      save:
        existingClaim: ark-save
        mountPath: /arkserver/ShooterGame/Saved


    # Startup and Liveness probe values
    # Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
    startupProbe:
      # 120s + 60*10s = 720s max
      initialDelaySeconds: 120
      failureThreshold: 60
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    livenessProbe:
      # unhealthy after max 3*10s = 30s
      initialDelaySeconds: 10
      failureThreshold: 3
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1

    serviceAccount:
      # Specifies whether a service account should be created
      create: true
      # Annotations to add to the service account
      annotations: {}
      # The name of the service account to use.
      # If not set and create is true, a name is generated using the fullname template
      name: ""

    extraInitContainers: []
    extraVolumeMounts: []
    extraVolumes: []
