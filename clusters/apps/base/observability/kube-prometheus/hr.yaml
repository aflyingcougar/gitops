# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack
---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus
  namespace: observability
spec:
  interval: 5m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 18.0.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
  values:
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: false
        general: true
        k8s: true
        kubeApiserver: true
        kubeApiserverAvailability: true
        kubeApiserverError: true
        kubeApiserverSlos: true
        kubelet: false
        kubePrometheusGeneral: true
        kubePrometheusNodeAlerting: true
        kubePrometheusNodeRecording: true
        kubernetesAbsent: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeScheduler: true
        kubeStateMetrics: true
        network: true
        node: true
        prometheus: true
        prometheusOperator: true
        time: true
      appNamespacesTarget: ".*"
      labels: {}
      annotations: {}
      additionalRuleLabels: {}
    additionalPrometheusRulesMap: {}
    global:
      rbac:
        create: true
        pspEnabled: true
        pspAnnotations: {}
    alertmanager:
      enabled: true
      annotations:
        k8s-app: alertmanager
      config:
        global:
          resolve_timeout: 5m
          smtp_smarthost: "${SMTP_HOST}:${SMTP_STARTTLS_PORT}"
          smtp_from: "${SMTP_FROM}"
          smtp_auth_username: "${SMTP_USERNAME}"
          smtp_auth_password: "${SMTP_PASSWORD}"
          slack_api_url: "${ALERT_DISCORD_WEBHOOK_URL}/slack"
        route:
          group_by: ['job', 'alertname']
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 24h
          receiver: 'discord'
          routes:
            - match:
                alertname: Watchdog
              receiver: 'null'
            - match_re:
                severity: critical|warning
              continue: true
              receiver: 'discord'
        receivers:
          - name: 'null'
          - name: 'discord'
            slack_configs:
              - channel: "#monitoring_alerts"
                icon_url: https://avatars3.githubusercontent.com/u/3380462
                send_resolved: true
                title: |-
                  [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }}{{ else }}{{ .CommonLabels.alertname }}{{ end }}
                text: >-
                  {{ range .Alerts -}}
                    **Alert:** {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
                  **Description:** {{ if ne .Annotations.description ""}}{{ .Annotations.description }}{{else}}N/A{{ end }}
                  **Details:**
                    {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                    {{ end }}
                  {{ end }}
        templates:
        - '/etc/alertmanager/config/*.tmpl'
      tplConfig: false
      templateFiles: {}
      ingress:
        enabled: true
        ingressClassName: internal
        annotations: {}
        labels: {}
        hosts:
          - "alerts.${INTERNAL_DOMAIN}"
        paths:
          - /
        pathType: ImplementationSpecific
        tls: []
      secret:
        annotations: {}
      ingressPerReplica:
        enabled: false
      service:
        annotations: {}
        labels: {}
        clusterIP: ""
        port: 9093
        targetPort: 9093
        nodePort: 30903
        additionalPorts: []
        externalIPs: []
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        type: ClusterIP
      servicePerReplica:
        enabled: false
      serviceMonitor:
        selfMonitor: true
      alertmanagerSpec:
        logFormat: logfmt
        logLevel: info
        replicas: 1
        retention: 120h
        storage: {}
        externalUrl: "alerts.${INTERNAL_DOMAIN}"
        routePrefix: /
        paused: false
        nodeSelector: {}
        resources: {}
    grafana:
      enabled: true
      defaultDashboardsEnabled: true
      adminPassword: "${GRAFANA_PASSWORD}"
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: external
          external-dns.alpha.kubernetes.io/target: dyn4.crutonjohn.com
          external-dns/public: true
        labels: {}
        hosts:
          - "grafana.${INTERNAL_DOMAIN}"
        path: /
        tls: []
      grafana.ini:
        server:
          root_url: "https://grafana.${INTERNAL_DOMAIN}"
        auth.basic:
          disable_login_form: false
        auth.generic_oauth:
          enabled: true
          allow_sign_up: true
            # team_ids:
            # allowed_organizations:
          name: Auth0
          client_id: "${AUTH0_CLIENT_ID}"
          client_secret: "${AUTH0_CLIENT_SECRET}"
          scopes: openid profile email
          auth_url: "https://${AUTH0_DOMAIN}.us.auth0.com/authorize"
          token_url: "https://${AUTH0_DOMAIN}.us.auth0.com/oauth/token"
          api_url: "https://${AUTH0_DOMAIN}.us.auth0.com/userinfo"
          # role_attribute_path: role
        smtp:
          enabled: true
          host: "${SMTP_HOST}:${SMTP_PORT}"
          user: "${SMTP_USERNAME}"
          password: "${SMTP_PASSWORD}"
          from_address: "${SMTP_FROM}"
          from_name: Grafana
      sidecar:
        dashboards:
          enabled: true
          label: grafana_dashboard
      serviceMonitor:
        interval: ""
        selfMonitor: true
        path: "/metrics"
    kubeApiServer:
      enabled: true
      tlsConfig:
        serverName: kubernetes
        insecureSkipVerify: true
    kubelet:
      enabled: false
    kubeControllerManager:
      enabled: false
    coreDns:
      enabled: true
      service:
        port: 9153
        targetPort: 9153
        selector:
          k8s-app: kube-dns
    kubeEtcd:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeProxy:
      enabled: false
    kubeStateMetrics:
      enabled: true
    kube-state-metrics:
      namespaceOverride: ""
      rbac:
        create: true
      podSecurityPolicy:
        enabled: true
    nodeExporter:
      enabled: true
    prometheus-node-exporter:
      namespaceOverride: ""
      podLabels:
        jobLabel: node-exporter
      extraArgs:
        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
        - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
    prometheusOperator:
      enabled: true
      kubeletService:
        enabled: false
        namespace: kube-system
    prometheus:
      enabled: true
      annotations: {}
      ingress:
        enabled: true
        ingressClassName: internal
        annotations: {}
        labels: {}
        hosts:
          - "prometheus.${INTERNAL_DOMAIN}"
        paths:
          - /
        pathType: ImplementationSpecific
      prometheusSpec:
        scrapeInterval: ""
        scrapeTimeout: ""
        retention: 30d
        retentionSize: "40GB"
        replicas: 1
        logLevel: info
        logFormat: logfmt
        routePrefix: /
        podMetadata:
          labels:
            app: prometheus
            k8s-app: prometheus
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi
            selector: {}
        additionalScrapeConfigs: []
        additionalScrapeConfigsSecret: {}
        additionalPrometheusSecretsAnnotations: {}
        additionalAlertManagerConfigs: []
        additionalAlertManagerConfigsSecret: {}
        additionalAlertRelabelConfigs: []
        securityContext:
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 1000
          fsGroup: 2000
        priorityClassName: ""
        portName: "web"
