---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus
  namespace: observability
  annotations:
    fluxcd.io/ignore: "false"
    fluxcd.io/automated: "true"
spec:
  releaseName: kube-prometheus-stack
  helmVersion: v3
  chart:
    repository: https://prometheus-community.github.io/helm-charts
    name: kube-prometheus-stack
    version: 9.4.10
  values:
    defaultRules:
      rules:
        alertmanager: true
        etcd: true
        general: true
        k8s: true
        kubeApiserver: true
        kubeApiserverAvailability: true
        kubeApiserverError: true
        kubeApiserverSlos: true
        kubelet: true
        kubePrometheusGeneral: true
        kubePrometheusNodeAlerting: true
        kubePrometheusNodeRecording: true
        kubernetesAbsent: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeScheduler: true
        kubeStateMetrics: true
        network: true
        node: true
        prometheus: true
        prometheusOperator: true
        time: true
    ## Provide custom recording or alerting rules to be deployed into the cluster.
    ##
    additionalPrometheusRulesMap: {}
    #  rule-name:
    #    groups:
    #    - name: my_group
    #      rules:
    #      - record: my_record
    #        expr: 100 * my_record
    alertmanager:
      enabled: true
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "internal"
          cert-manager.io/cluster-issuer: letsencrypt-prod
        hosts:
        - "alerts.crutonjohn.com"
        tls:
        - hosts:
          - "alerts.crutonjohn.com"
          secretName: alerts-crutonjohn-com
      templateFiles:
        ################
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 10Gi
            selector: {}
    grafana:
      enabled: true
      defaultDashboardsEnabled: true
      env:
        GF_EXPLORE_ENABLED: true
        GF_PANELS_DISABLE_SANITIZE_HTML: true
      persistence:
        enabled: true
        storageClassName: "longhorn-weekly"
        size: 10Gi
        accessModes:
        - ReadWriteOnce
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "internal"
          cert-manager.io/cluster-issuer: letsencrypt-prod
        hosts:
        - "grafana.crutonjohn.com"
        tls:
        - hosts:
          - "grafana.crutonjohn.com"
          secretName: grafana-crutonjohn-com
      plugins:
      - natel-discrete-panel
      - pr0ps-trackmap-panel
      - grafana-piechart-panel
      - vonage-status-panel
      - grafana-worldmap-panel
      - grafana-clock-panel
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            allowUiUpdates: true
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          flux:
            url: https://raw.githubusercontent.com/fluxcd/flux/master/chart/flux/dashboards/flux.json
            datasource: Prometheus
          helm-operator:
            url: https://raw.githubusercontent.com/fluxcd/helm-operator/master/chart/helm-operator/dashboards/helm-operator.json
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/13116
          ups-dashboard:
            gnetId: 13116
            revision: 1
            datasource: Prometheus
          unifi-client-dpi:
            gnetId: 11310
            revision: 4
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/11315
          unifi-client-insights:
            gnetId: 11315
            revision: 8
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/11311
          unifi-network-sites:
            gnetId: 11311
            revision: 4
            datasource: Prometheus
          # Ref: https://grafana.com/grafana/dashboards/11314
          unifi-uap-insights:
            gnetId: 11314
            revision: 9
            datasource: Prometheus
          # version-checker:
          #   gnetId: 12833
          #   revision: 1
          #   datasource: Prometheus
      grafana.ini:
        paths:
          data: /var/lib/grafana/data
          logs: /var/log/grafana
          plugins: /var/lib/grafana/plugins
          provisioning: /etc/grafana/provisioning
        analytics:
          check_for_updates: true
        log:
          mode: console
        grafana_net:
          url: https://grafana.net
      sidecar:
        dashboards:
          enabled: true
          label: grafana_dashboard
        datasources:
          enabled: true
          defaultDatasourceEnabled: true
      additionalDataSources:
      - name: loki
        type: loki
        access: proxy
        url: http://loki.logging.svc.cluster.local:3100
      serviceMonitor:
        interval: ""
        selfMonitor: true
        metricRelabelings: []
        relabelings: []
    kubeApiServer:
      enabled: true
    kubelet:
      enabled: true
      serviceMonitor:
        metricRelabelings:
        - action: replace
          sourceLabels:
          - node
          targetLabel: instance
    kubeControllerManager:
      enabled: true
    coreDns:
      enabled: true
    kubeEtcd:
      enabled: true
    kubeScheduler:
      enabled: true
    kubeProxy:
      enabled: true
    kubeStateMetrics:
      enabled: true
    kube-state-metrics:
      namespaceOverride: ""
      rbac:
        create: true
      podSecurityPolicy:
        enabled: true
    nodeExporter:
      enabled: true
    prometheus-node-exporter:
      namespaceOverride: ""
      podLabels:
        ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
        ##
        jobLabel: node-exporter
      extraArgs:
      - --collector.filesystem.ignored-mount-points=^/(atlas|pandora|dev|proc|sys|var/lib/docker/.+)($|/)
      - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
    prometheusOperator:
      enabled: true

      # If true prometheus operator will create and update its CRDs on startup
      # Only for prometheusOperator.image.tag < v0.39.0
      manageCrds: true

      tlsProxy:
        enabled: true
        image:
          repository: squareup/ghostunnel
          tag: v1.5.2
          sha: ""
          pullPolicy: IfNotPresent
        resources: {}

      ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
      ## rules from making their way into prometheus and potentially preventing the container from starting
      admissionWebhooks:
        failurePolicy: Fail
        enabled: true
        ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
        ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
        ## certs ahead of time if you wish.
        ##
        patch:
          enabled: true
          image:
            repository: jettech/kube-webhook-certgen
            tag: v1.2.1
            sha: ""
            pullPolicy: IfNotPresent
          resources: {}
          ## Provide a priority class name to the webhook patching job
          ##
          priorityClassName: ""
          podAnnotations: {}
          nodeSelector: {}
          affinity: {}
          tolerations: []

      ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
      ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
      ##
      namespaces: {}
        # releaseNamespace: true
        # additional:
        # - kube-system

      ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
      ##
      denyNamespaces: []

      ## Service account for Alertmanager to use.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
      ##
      serviceAccount:
        create: true
        name: ""

      ## Configuration for Prometheus operator service
      ##
      service:
        annotations: {}
        labels: {}
        clusterIP: ""

      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
        nodePort: 30080

        nodePortTls: 30443

      ## Additional ports to open for Prometheus service
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
      ##
        additionalPorts: []

      ## Loadbalancer IP
      ## Only use if service.type is "loadbalancer"
      ##
        loadBalancerIP: ""
        loadBalancerSourceRanges: []

      ## Service type
      ## NodePort, ClusterIP, loadbalancer
      ##
        type: ClusterIP

        ## List of IP addresses at which the Prometheus server service is available
        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
        ##
        externalIPs: []

      ## Deploy CRDs used by Prometheus Operator.
      ##
      createCustomResource: true

      ## Attempt to clean up CRDs created by Prometheus Operator.
      ##
      cleanupCustomResource: false

      ## Labels to add to the operator pod
      ##
      podLabels: {}

      ## Annotations to add to the operator pod
      ##
      podAnnotations: {}

      ## Assign a PriorityClassName to pods if set
      # priorityClassName: ""

      ## Define Log Format
      # Use logfmt (default) or json-formatted logging
      # logFormat: logfmt

      ## Decrease log verbosity to errors only
      # logLevel: error

      ## If true, the operator will create and maintain a service for scraping kubelets
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/helm/prometheus-operator/README.md
      ##
      kubeletService:
        enabled: true
        namespace: kube-system

      ## Create a servicemonitor for the operator
      ##
      serviceMonitor:
        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""
        ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.
        scrapeTimeout: ""
        selfMonitor: true

        ## 	metric relabel configs to apply to samples before ingestion.
        ##
        metricRelabelings: []
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]

        # 	relabel configs to apply to samples before ingestion.
        ##
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace

      ## Resource limits & requests
      ##
      resources: {}
      # limits:
      #   cpu: 200m
      #   memory: 200Mi
      # requests:
      #   cpu: 100m
      #   memory: 100Mi

      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
      ##
      hostNetwork: false

      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## Tolerations for use with node taints
      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      ##
      tolerations: []
      # - key: "key"
      #   operator: "Equal"
      #   value: "value"
      #   effect: "NoSchedule"

      ## Assign custom affinity rules to the prometheus operator
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      affinity: {}
        # nodeAffinity:
        #   requiredDuringSchedulingIgnoredDuringExecution:
        #     nodeSelectorTerms:
        #     - matchExpressions:
        #       - key: kubernetes.io/e2e-az-name
        #         operator: In
        #         values:
        #         - e2e-az1
        #         - e2e-az2

      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534

      ## Prometheus-operator image
      ##
      image:
        repository: quay.io/coreos/prometheus-operator
        tag: v0.42.1-arm
        sha: ""
        pullPolicy: IfNotPresent

      ## Prometheus-config-reloader image to use for config and rule reloading
      ##
      prometheusConfigReloaderImage:
        repository: quay.io/coreos/prometheus-config-reloader
        tag: v0.38.1
        sha: ""

      ## Set the prometheus config reloader side-car CPU limit
      ##
      configReloaderCpu: 100m

      ## Set the prometheus config reloader side-car memory limit
      ##
      configReloaderMemory: 25Mi

      ## Set a Field Selector to filter watched secrets
      ##
      secretFieldSelector: ""

      ## kubectl image to use when cleaning up
      ##
      kubectlImage:
        repository: docker.io/bitnami/kubectl
        tag: 1.16.15
        sha: ""
        pullPolicy: IfNotPresent

    ## Deploy a Prometheus instance
    ##
    prometheus:
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "internal"
          cert-manager.io/cluster-issuer: letsencrypt-prod
        hosts: []
        tls:
        - hosts:
          - "prometheus.crutonjohn.com"
          secretName: prometheus-crutonjohn-com
      prometheusSpec:
        disableCompaction: false
        apiserverConfig: {}
        scrapeInterval: ""
        evaluationInterval: ""
        listenLocal: false
        enableAdminAPI: false
        image:
          repository: quay.io/prometheus/prometheus
          tag: v2.18.2
          sha: ""
        tolerations: []
        alertingEndpoints: []
        externalLabels: {}
        replicaExternalLabelName: ""
        replicaExternalLabelNameClear: false
        prometheusExternalLabelName: ""
        prometheusExternalLabelNameClear: false
        externalUrl: ""
        nodeSelector: {}
        secrets: []
        configMaps: []
        query: {}
        ruleNamespaceSelector: {}
        ruleSelectorNilUsesHelmValues: true
        ruleSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: true
        serviceMonitorSelector: {}
        serviceMonitorNamespaceSelector: {}
        podMonitorSelectorNilUsesHelmValues: true
        podMonitorSelector: {}
        podMonitorNamespaceSelector: {}
        retention: 60d
        retentionSize: ""
        walCompression: false
        paused: false
        replicas: 1
        logLevel: info
        logFormat: logfmt
        routePrefix: /
        podMetadata: {}
        podAntiAffinity: ""
        podAntiAffinityTopologyKey: kubernetes.io/hostname
        affinity: {}
        remoteRead: []
        remoteWrite: []
        remoteWriteDashboards: false
        resources: {}
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: "longhorn"
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi
        additionalScrapeConfigs: []
        additionalScrapeConfigsSecret: {}
        additionalPrometheusSecretsAnnotations: {}
        additionalAlertManagerConfigs: []
        additionalAlertRelabelConfigs: []
        priorityClassName: ""
        thanos: {}
        containers: []
        initContainers: []
        portName: "web"
      additionalServiceMonitors: []
      additionalPodMonitors: []
  valueFileSecrets:
  - name: "kube-prometheus-stack-helm-values"
